{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007fe0c2",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ba3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Import SMF110 modules\n",
    "from smf110_binary_parser import SMF110BinaryParser\n",
    "from smf110_parser import SMF110Parser\n",
    "from smf110_analysis import SMF110Analysis\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa8676",
   "metadata": {},
   "source": [
    "## Step 2: Configure Binary Dump File Path\n",
    "\n",
    "**Update the path below to point to your SMF 110 binary dump file.**\n",
    "\n",
    "### How to Obtain SMF 110 Dump:\n",
    "```jcl\n",
    "//SMFDUMP  JOB  ...\n",
    "//STEP1    EXEC PGM=IFASMFDP\n",
    "//INDD     DD   DSN=SYS1.MAN1,DISP=SHR\n",
    "//OUTDD    DD   DSN=USER.SMF110.DUMP,\n",
    "//              DISP=(NEW,CATLG,DELETE),\n",
    "//              SPACE=(CYL,(10,5),RLSE),\n",
    "//              DCB=(RECFM=VBS,LRECL=32760,BLKSIZE=32764)\n",
    "//SYSIN    DD   *\n",
    "  INDD(INDD,OPTIONS(DUMP))\n",
    "  OUTDD(OUTDD,TYPE(110))\n",
    "  DATE(2024335,2024336)\n",
    "/*\n",
    "```\n",
    "\n",
    "Then FTP in **binary mode**:\n",
    "```\n",
    "ftp mainframe.example.com\n",
    "> binary\n",
    "> get 'USER.SMF110.DUMP' smf110.dump\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to your actual SMF 110 binary dump file\n",
    "DUMP_FILE = r\"C:\\path\\to\\your\\smf110.dump\"\n",
    "\n",
    "# Or use a sample/test file if you have one\n",
    "# DUMP_FILE = r\"sample_smf110.bin\"\n",
    "\n",
    "dump_path = Path(DUMP_FILE)\n",
    "\n",
    "if dump_path.exists():\n",
    "    print(f\"âœ“ Found dump file: {dump_path}\")\n",
    "    print(f\"  File size: {dump_path.stat().st_size:,} bytes\")\n",
    "else:\n",
    "    print(f\"âš  Dump file not found: {dump_path}\")\n",
    "    print(\"  Please update DUMP_FILE path above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7ffbb",
   "metadata": {},
   "source": [
    "## Step 3: Parse Binary Dump\n",
    "\n",
    "This step:\n",
    "- Reads the binary SMF 110 dump\n",
    "- Converts EBCDIC to ASCII\n",
    "- Unpacks binary fields (big-endian)\n",
    "- Organizes records by subtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize binary parser\n",
    "print(\"Parsing SMF 110 binary dump...\\n\")\n",
    "parser = SMF110BinaryParser(DUMP_FILE)\n",
    "\n",
    "# Parse all records\n",
    "records_by_subtype = parser.parse_dump()\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_records = sum(len(recs) for recs in records_by_subtype.values())\n",
    "print(f\"Total records parsed: {total_records}\\n\")\n",
    "\n",
    "subtype_names = {\n",
    "    1: \"Transaction Statistics\",\n",
    "    2: \"File Statistics\",\n",
    "    3: \"Program Statistics\",\n",
    "    4: \"Terminal Statistics\",\n",
    "    5: \"Storage Statistics\",\n",
    "    6: \"Dispatcher Statistics\",\n",
    "    7: \"Loader Statistics\",\n",
    "    8: \"Temporary Storage\",\n",
    "    9: \"Transient Data\",\n",
    "    10: \"Journal Statistics\",\n",
    "    11: \"Database Statistics\",\n",
    "    12: \"MQ Statistics\",\n",
    "    13: \"Web Services\",\n",
    "    14: \"ISC Statistics\",\n",
    "    15: \"Coupling Facility\"\n",
    "}\n",
    "\n",
    "for subtype in range(1, 16):\n",
    "    count = len(records_by_subtype.get(subtype, []))\n",
    "    if count > 0:\n",
    "        name = subtype_names.get(subtype, f\"Unknown {subtype}\")\n",
    "        print(f\"  Subtype {subtype:2d} ({name:25s}): {count:5d} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53597b",
   "metadata": {},
   "source": [
    "## Step 4: Transaction Analysis (Subtype 1)\n",
    "\n",
    "Analyze CICS transaction performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transaction records\n",
    "transaction_records = records_by_subtype.get(1, [])\n",
    "\n",
    "if transaction_records:\n",
    "    print(f\"Analyzing {len(transaction_records)} transaction records...\\n\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_trans = pd.DataFrame([rec.to_dict() for rec in transaction_records])\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"Sample Transaction Records:\")\n",
    "    print(df_trans[['transaction_id', 'program_name', 'cpu_time', 'response_time', \n",
    "                     'transaction_count', 'completed', 'abended']].head(10))\n",
    "    \n",
    "    # Key statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRANSACTION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total transactions: {df_trans['transaction_count'].sum():,}\")\n",
    "    print(f\"Completed: {df_trans['completed'].sum():,}\")\n",
    "    print(f\"Abended: {df_trans['abended'].sum():,}\")\n",
    "    print(f\"Avg CPU time: {df_trans['cpu_time'].mean():.2f} ms\")\n",
    "    print(f\"Avg response time: {df_trans['response_time'].mean():.2f} ms\")\n",
    "    print(f\"Max response time: {df_trans['response_time'].max():.2f} ms\")\n",
    "else:\n",
    "    print(\"No transaction records found in dump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460fe09",
   "metadata": {},
   "source": [
    "## Step 5: File Statistics Analysis (Subtype 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77838fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file records\n",
    "file_records = records_by_subtype.get(2, [])\n",
    "\n",
    "if file_records:\n",
    "    print(f\"Analyzing {len(file_records)} file records...\\n\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_files = pd.DataFrame([rec.to_dict() for rec in file_records])\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"Sample File Records:\")\n",
    "    print(df_files[['file_name', 'file_type', 'reads', 'writes', \n",
    "                     'buffer_hits', 'buffer_misses', 'io_errors']].head(10))\n",
    "    \n",
    "    # Calculate buffer hit ratio\n",
    "    df_files['hit_ratio'] = (\n",
    "        df_files['buffer_hits'] / \n",
    "        (df_files['buffer_hits'] + df_files['buffer_misses']) * 100\n",
    "    ).fillna(0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FILE STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total files: {len(df_files)}\")\n",
    "    print(f\"Total reads: {df_files['reads'].sum():,}\")\n",
    "    print(f\"Total writes: {df_files['writes'].sum():,}\")\n",
    "    print(f\"Avg buffer hit ratio: {df_files['hit_ratio'].mean():.2f}%\")\n",
    "    print(f\"Total I/O errors: {df_files['io_errors'].sum():,}\")\n",
    "else:\n",
    "    print(\"No file records found in dump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f85477",
   "metadata": {},
   "source": [
    "## Step 6: Visualization - Transaction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5306fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if transaction_records and len(transaction_records) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('CICS Transaction Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Top transactions by count\n",
    "    top_trans = df_trans.nlargest(10, 'transaction_count')\n",
    "    axes[0, 0].barh(top_trans['transaction_id'], top_trans['transaction_count'])\n",
    "    axes[0, 0].set_xlabel('Transaction Count')\n",
    "    axes[0, 0].set_ylabel('Transaction ID')\n",
    "    axes[0, 0].set_title('Top 10 Transactions by Volume')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    \n",
    "    # 2. CPU time distribution\n",
    "    axes[0, 1].hist(df_trans['cpu_time'], bins=30, edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('CPU Time (ms)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('CPU Time Distribution')\n",
    "    \n",
    "    # 3. Response time by transaction\n",
    "    top_response = df_trans.nlargest(10, 'response_time')\n",
    "    axes[1, 0].barh(top_response['transaction_id'], top_response['response_time'], color='coral')\n",
    "    axes[1, 0].set_xlabel('Response Time (ms)')\n",
    "    axes[1, 0].set_ylabel('Transaction ID')\n",
    "    axes[1, 0].set_title('Top 10 Slowest Transactions')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    \n",
    "    # 4. Abend rate\n",
    "    abend_data = df_trans[df_trans['abended'] > 0].nlargest(10, 'abended')\n",
    "    if len(abend_data) > 0:\n",
    "        axes[1, 1].bar(abend_data['transaction_id'], abend_data['abended'], color='red')\n",
    "        axes[1, 1].set_xlabel('Transaction ID')\n",
    "        axes[1, 1].set_ylabel('Abend Count')\n",
    "        axes[1, 1].set_title('Transactions with Abends')\n",
    "        plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No Abends Detected', \n",
    "                        ha='center', va='center', fontsize=14, color='green')\n",
    "        axes[1, 1].set_title('Abend Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No transaction data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76e699",
   "metadata": {},
   "source": [
    "## Step 7: Visualization - File I/O Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165b4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "if file_records and len(file_records) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('CICS File I/O Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Top files by I/O\n",
    "    df_files['total_io'] = df_files['reads'] + df_files['writes']\n",
    "    top_io = df_files.nlargest(10, 'total_io')\n",
    "    axes[0, 0].barh(top_io['file_name'], top_io['total_io'])\n",
    "    axes[0, 0].set_xlabel('Total I/O Operations')\n",
    "    axes[0, 0].set_ylabel('File Name')\n",
    "    axes[0, 0].set_title('Top 10 Files by I/O Activity')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    \n",
    "    # 2. Buffer hit ratio\n",
    "    top_buffer = df_files.nlargest(10, 'buffer_requests')\n",
    "    axes[0, 1].barh(top_buffer['file_name'], top_buffer['hit_ratio'], color='green')\n",
    "    axes[0, 1].set_xlabel('Buffer Hit Ratio (%)')\n",
    "    axes[0, 1].set_ylabel('File Name')\n",
    "    axes[0, 1].set_title('Buffer Hit Ratio (Higher is Better)')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].set_xlim(0, 100)\n",
    "    \n",
    "    # 3. Read vs Write comparison\n",
    "    top_files = df_files.nlargest(10, 'total_io')\n",
    "    x = range(len(top_files))\n",
    "    width = 0.35\n",
    "    axes[1, 0].bar([i - width/2 for i in x], top_files['reads'], width, label='Reads', color='skyblue')\n",
    "    axes[1, 0].bar([i + width/2 for i in x], top_files['writes'], width, label='Writes', color='orange')\n",
    "    axes[1, 0].set_xlabel('File')\n",
    "    axes[1, 0].set_ylabel('Operations')\n",
    "    axes[1, 0].set_title('Read vs Write Operations')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(top_files['file_name'], rotation=45, ha='right')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. I/O errors\n",
    "    error_files = df_files[df_files['io_errors'] > 0]\n",
    "    if len(error_files) > 0:\n",
    "        axes[1, 1].bar(error_files['file_name'], error_files['io_errors'], color='red')\n",
    "        axes[1, 1].set_xlabel('File Name')\n",
    "        axes[1, 1].set_ylabel('Error Count')\n",
    "        axes[1, 1].set_title('Files with I/O Errors')\n",
    "        plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No I/O Errors Detected', \n",
    "                        ha='center', va='center', fontsize=14, color='green')\n",
    "        axes[1, 1].set_title('I/O Error Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No file data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f81e4",
   "metadata": {},
   "source": [
    "## Step 8: Export Reports\n",
    "\n",
    "Generate CSV and JSON reports for all subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ee841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reports directory\n",
    "reports_dir = Path('reports')\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Initialize parser\n",
    "report_parser = SMF110Parser()\n",
    "\n",
    "print(\"Generating reports...\\n\")\n",
    "\n",
    "generated_files = []\n",
    "\n",
    "# Generate reports for each subtype\n",
    "for subtype, records in records_by_subtype.items():\n",
    "    if records:\n",
    "        subtype_name = subtype_names.get(subtype, f\"Type{subtype}\")\n",
    "        \n",
    "        # CSV report\n",
    "        csv_file = reports_dir / f\"smf110_type{subtype}_{subtype_name.replace(' ', '_').lower()}.csv\"\n",
    "        report_parser.save_csv_report(records, csv_file)\n",
    "        generated_files.append(csv_file)\n",
    "        \n",
    "        # JSON report\n",
    "        json_file = reports_dir / f\"smf110_type{subtype}_{subtype_name.replace(' ', '_').lower()}.json\"\n",
    "        report_parser.save_json_report(records, json_file)\n",
    "        generated_files.append(json_file)\n",
    "        \n",
    "        print(f\"âœ“ Subtype {subtype:2d} ({subtype_name:25s}): {len(records):4d} records\")\n",
    "\n",
    "print(f\"\\nâœ“ Generated {len(generated_files)} report files in {reports_dir}/\")\n",
    "print(\"\\nReport files:\")\n",
    "for f in sorted(generated_files):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223f822",
   "metadata": {},
   "source": [
    "## Step 9: Advanced Analysis - All Subtypes Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_data = []\n",
    "\n",
    "for subtype in range(1, 16):\n",
    "    records = records_by_subtype.get(subtype, [])\n",
    "    summary_data.append({\n",
    "        'Subtype': subtype,\n",
    "        'Name': subtype_names.get(subtype, 'Unknown'),\n",
    "        'Record Count': len(records),\n",
    "        'Status': 'âœ“' if len(records) > 0 else '-'\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SMF 110 SUBTYPE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal Records: {df_summary['Record Count'].sum():,}\")\n",
    "print(f\"Active Subtypes: {(df_summary['Record Count'] > 0).sum()} / 15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e933b68",
   "metadata": {},
   "source": [
    "## Step 10: Export Summary Dashboard\n",
    "\n",
    "Create a comprehensive visual dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "fig.suptitle('SMF 110 CICS Statistics - Complete Analysis Dashboard', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# Create grid\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Subtype distribution (top left, spanning 2 columns)\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "active_subtypes = df_summary[df_summary['Record Count'] > 0]\n",
    "ax1.barh(active_subtypes['Name'], active_subtypes['Record Count'], color='steelblue')\n",
    "ax1.set_xlabel('Number of Records', fontsize=11)\n",
    "ax1.set_title('Records by Subtype', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "# Add remaining visualizations based on available data\n",
    "if transaction_records:\n",
    "    # 2. Transaction CPU distribution\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    ax2.hist(df_trans['cpu_time'], bins=20, color='orange', edgecolor='black')\n",
    "    ax2.set_xlabel('CPU Time (ms)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Transaction CPU Time', fontweight='bold')\n",
    "\n",
    "if file_records:\n",
    "    # 3. File buffer efficiency\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax3.hist(df_files['hit_ratio'], bins=20, color='green', edgecolor='black')\n",
    "    ax3.set_xlabel('Buffer Hit Ratio (%)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('File Buffer Efficiency', fontweight='bold')\n",
    "\n",
    "# 4. Overall statistics text box\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "ax4.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "OVERALL STATISTICS\n",
    "\n",
    "Total Records: {total_records:,}\n",
    "Active Subtypes: {(df_summary['Record Count'] > 0).sum()}/15\n",
    "\n",
    "Analysis Date:\n",
    "{datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "Dump File:\n",
    "{Path(DUMP_FILE).name}\n",
    "\"\"\"\n",
    "ax4.text(0.1, 0.5, stats_text, fontsize=10, family='monospace', \n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "# 5. Top transactions pie chart\n",
    "if transaction_records:\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    top5_trans = df_trans.nlargest(5, 'transaction_count')\n",
    "    ax5.pie(top5_trans['transaction_count'], labels=top5_trans['transaction_id'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    ax5.set_title('Top 5 Transactions', fontweight='bold')\n",
    "\n",
    "# 6. File I/O type distribution\n",
    "if file_records:\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    io_types = ['Reads', 'Writes', 'Updates', 'Deletes']\n",
    "    io_counts = [\n",
    "        df_files['reads'].sum(),\n",
    "        df_files['writes'].sum(),\n",
    "        df_files['updates'].sum(),\n",
    "        df_files['deletes'].sum()\n",
    "    ]\n",
    "    ax6.pie(io_counts, labels=io_types, autopct='%1.1f%%', startangle=90)\n",
    "    ax6.set_title('File I/O Operations', fontweight='bold')\n",
    "\n",
    "# 7. Success rate indicator\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.axis('off')\n",
    "if transaction_records:\n",
    "    success_rate = (df_trans['completed'].sum() / df_trans['transaction_count'].sum() * 100) if df_trans['transaction_count'].sum() > 0 else 0\n",
    "    status_text = f\"\"\"\n",
    "HEALTH METRICS\n",
    "\n",
    "Transaction Success Rate:\n",
    "{success_rate:.2f}%\n",
    "\n",
    "Total Abends:\n",
    "{df_trans['abended'].sum():,}\n",
    "    \"\"\"\n",
    "    color = 'lightgreen' if success_rate > 95 else 'lightyellow' if success_rate > 90 else 'lightcoral'\n",
    "else:\n",
    "    status_text = \"\\n\\nNo transaction data\\navailable\"\n",
    "    color = 'lightgray'\n",
    "\n",
    "ax7.text(0.1, 0.5, status_text, fontsize=11, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor=color, alpha=0.5))\n",
    "\n",
    "# Save dashboard\n",
    "dashboard_file = reports_dir / 'smf110_dashboard.png'\n",
    "plt.savefig(dashboard_file, dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nâœ“ Dashboard saved: {dashboard_file}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb413c52",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides complete analysis of SMF 110 binary dumps:\n",
    "\n",
    "### âœ“ Completed\n",
    "- Parsed binary dump with EBCDIC conversion\n",
    "- Analyzed all 15 CICS statistics subtypes\n",
    "- Generated interactive visualizations\n",
    "- Exported CSV/JSON reports\n",
    "- Created comprehensive dashboard\n",
    "\n",
    "### ðŸ“Š Outputs\n",
    "- **CSV Reports**: One per subtype in `reports/`\n",
    "- **JSON Reports**: One per subtype in `reports/`\n",
    "- **Dashboard**: `reports/smf110_dashboard.png`\n",
    "- **Interactive Charts**: Displayed inline\n",
    "\n",
    "### ðŸ“– Documentation\n",
    "See `BINARY_DUMP_GUIDE.md` for:\n",
    "- SMF 110 binary format details\n",
    "- JCL examples for dump extraction\n",
    "- FTP transfer instructions\n",
    "- Troubleshooting guide"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
