{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fe4115",
   "metadata": {},
   "source": [
    "# SMF 30 Binary Analysis - Complete Guide\n",
    "\n",
    "This notebook demonstrates how to analyze **raw SMF 30 binary dumps** from z/OS mainframe using the SMF30 binary parser.\n",
    "\n",
    "## Overview\n",
    "\n",
    "SMF Type 30 records contain **Job and Step completion statistics** including:\n",
    "- CPU time and service units\n",
    "- Memory usage (real and virtual)\n",
    "- I/O operations (EXCP counts)\n",
    "- Job timing information\n",
    "- Resource consumption metrics\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Raw SMF 30 dump file** from z/OS mainframe\n",
    "2. **Python libraries**: pandas, matplotlib, numpy\n",
    "3. **SMF30 binary parser** (included in this workspace)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b3534",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import struct\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import SMF30 modules from current workspace\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "from smf30_binary_parser import SMFBinaryParser\n",
    "from smf30_structures import *\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"✓ Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc195b",
   "metadata": {},
   "source": [
    "## 2. Load Raw Binary SMF 30 Dump\n",
    "\n",
    "Specify the path to your **raw SMF 30 binary dump** file from the mainframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f246b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your SMF 30 binary dump file path\n",
    "# Use small sample for faster execution, or \"dumpsample.bin\" for full 12.9MB dump\n",
    "dump_file = \"dumpsample_small.bin\"  # Small 100KB sample for quick testing\n",
    "\n",
    "# Verify file exists\n",
    "dump_path = Path(dump_file)\n",
    "if dump_path.exists():\n",
    "    file_size_mb = dump_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"✓ Found dump file: {dump_path}\")\n",
    "    print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"⚠ Dump file not found: {dump_file}\")\n",
    "    print(f\"  Please update the path to your SMF 30 binary dump\")\n",
    "    print(f\"\\n  To obtain SMF dumps from z/OS, see: BINARY_DUMP_GUIDE.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5349a",
   "metadata": {},
   "source": [
    "## 3. Parse Binary SMF 30 Records\n",
    "\n",
    "The parser automatically handles:\n",
    "- **RDW (Record Descriptor Word)** - Variable-length record headers\n",
    "- **EBCDIC to ASCII** conversion (code page 500)\n",
    "- **Big-endian binary** unpacking\n",
    "- **Subtype detection** at offset 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the binary parser\n",
    "parser = SMFBinaryParser(dump_file)\n",
    "\n",
    "# Parse all records from the dump\n",
    "print(\"Parsing SMF 30 binary dump...\")\n",
    "records_by_subtype = parser.parse_dump_file()\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "total_records = sum(len(records) for records in records_by_subtype.values())\n",
    "print(f\"Total records parsed: {total_records}\")\n",
    "print(\"\\nRecords by Subtype:\")\n",
    "for subtype, records in sorted(records_by_subtype.items()):\n",
    "    if records:\n",
    "        print(f\"  Subtype {subtype}: {len(records):4d} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf96a2",
   "metadata": {},
   "source": [
    "## 4. Convert to Pandas DataFrames\n",
    "\n",
    "Create DataFrames for each subtype for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert parsed records to DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "for subtype, records in records_by_subtype.items():\n",
    "    if records:\n",
    "        # Convert to dictionaries\n",
    "        data = [record.to_dict() for record in records]\n",
    "        df = pd.DataFrame(data)\n",
    "        dataframes[subtype] = df\n",
    "        print(f\"Subtype {subtype}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Store most common subtypes for analysis\n",
    "df_interval = dataframes.get(1, pd.DataFrame())  # Subtype 1: Interval\n",
    "df_step = dataframes.get(4, pd.DataFrame())      # Subtype 4: Step completion\n",
    "df_job = dataframes.get(5, pd.DataFrame())       # Subtype 5: Job completion\n",
    "\n",
    "print(f\"\\n✓ Created {len(dataframes)} DataFrames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de891f",
   "metadata": {},
   "source": [
    "## 5. Inspect Sample Records\n",
    "\n",
    "Let's examine the structure and data from parsed records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef62f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample from Job Completion records (Subtype 1)\n",
    "if not df_interval.empty:\n",
    "    print(\"Job Step Termination Records (Subtype 1) - Sample\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_interval[['job_name', 'program_name', 'step_name', 'cpu_time_ms', \n",
    "                       'elapsed_time_ms', 'excp_count']].head(10))\n",
    "    print(f\"\\nTotal records: {len(df_interval)}\")\n",
    "else:\n",
    "    print(\"No records found\")\n",
    "\n",
    "# Display Step Completion if available\n",
    "if not df_step.empty:\n",
    "    print(\"\\n\\nStep Completion Records (Subtype 4) - Sample\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_step[['job_name', 'step_name', 'program_name', 'cpu_time_ms']].head(5))\n",
    "    print(f\"\\nTotal Step records: {len(df_step)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd574b",
   "metadata": {},
   "source": [
    "## 6. Analyze CPU Usage\n",
    "\n",
    "Analyze CPU consumption across jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6e525",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_interval.empty:\n",
    "    # CPU statistics (convert ms to seconds)\n",
    "    df_interval['cpu_time_sec'] = df_interval['cpu_time_ms'] / 1000\n",
    "    print(\"CPU Usage Statistics\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total CPU time: {df_interval['cpu_time_sec'].sum():.2f} seconds\")\n",
    "    print(f\"Average CPU per job: {df_interval['cpu_time_sec'].mean():.2f} seconds\")\n",
    "    print(f\"Max CPU (single job): {df_interval['cpu_time_sec'].max():.2f} seconds\")\n",
    "    print(f\"Min CPU (single job): {df_interval['cpu_time_sec'].min():.2f} seconds\")\n",
    "    \n",
    "    # Top CPU consumers\n",
    "    print(\"\\n\\nTop 10 CPU Consuming Jobs\")\n",
    "    print(\"=\"*60)\n",
    "    top_cpu = df_interval.nlargest(10, 'cpu_time_ms')[['job_name', 'program_name', 'cpu_time_sec']]\n",
    "    print(top_cpu.to_string(index=False))\n",
    "else:\n",
    "    print(\"No job data available for CPU analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ef702",
   "metadata": {},
   "source": [
    "## 7. Visualize CPU Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_interval.empty:\n",
    "    # Convert ms to seconds for visualization\n",
    "    df_interval['cpu_time_sec'] = df_interval['cpu_time_ms'] / 1000\n",
    "    df_interval['elapsed_time_sec'] = df_interval['elapsed_time_ms'] / 1000\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. CPU Time Distribution\n",
    "    axes[0, 0].hist(df_interval['cpu_time_sec'], bins=50, color='steelblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('CPU Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('CPU Time Distribution Across Jobs')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Top 15 CPU Consumers\n",
    "    top15 = df_interval.nlargest(15, 'cpu_time_sec')\n",
    "    axes[0, 1].barh(range(len(top15)), top15['cpu_time_sec'], color='coral')\n",
    "    axes[0, 1].set_yticks(range(len(top15)))\n",
    "    axes[0, 1].set_yticklabels(top15['job_name'], fontsize=8)\n",
    "    axes[0, 1].set_xlabel('CPU Time (seconds)')\n",
    "    axes[0, 1].set_title('Top 15 CPU Consuming Jobs')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 3. CPU vs Elapsed Time\n",
    "    axes[1, 0].scatter(df_interval['elapsed_time_sec'], df_interval['cpu_time_sec'], \n",
    "                       alpha=0.6, s=30, color='green')\n",
    "    axes[1, 0].set_xlabel('Elapsed Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('CPU Time (seconds)')\n",
    "    axes[1, 0].set_title('CPU Time vs Elapsed Time')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. CPU by Program Name (top 10 programs)\n",
    "    if 'program_name' in df_interval.columns:\n",
    "        program_cpu = df_interval.groupby('program_name')['cpu_time_sec'].sum().nlargest(10)\n",
    "        axes[1, 1].bar(range(len(program_cpu)), program_cpu.values, color='purple')\n",
    "        axes[1, 1].set_xticks(range(len(program_cpu)))\n",
    "        axes[1, 1].set_xticklabels(program_cpu.index, rotation=45, ha='right', fontsize=8)\n",
    "        axes[1, 1].set_ylabel('Total CPU Time (seconds)')\n",
    "        axes[1, 1].set_title('CPU Usage by Program (Top 10)')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/cpu_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n✓ CPU visualization saved to: reports/cpu_analysis.png\")\n",
    "else:\n",
    "    print(\"No job data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec903d4",
   "metadata": {},
   "source": [
    "## 8. Analyze Memory Usage\n",
    "\n",
    "Examine real and virtual storage consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33bae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_job.empty and 'real_storage' in df_job.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Real Storage Usage\n",
    "    axes[0].hist(df_job['real_storage'] / 1024, bins=40, color='teal', edgecolor='black')\n",
    "    axes[0].set_xlabel('Real Storage (MB)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Real Storage Usage Distribution')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Virtual Storage Usage\n",
    "    if 'virtual_storage' in df_job.columns:\n",
    "        axes[1].hist(df_job['virtual_storage'] / 1024, bins=40, color='orange', edgecolor='black')\n",
    "        axes[1].set_xlabel('Virtual Storage (MB)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Virtual Storage Usage Distribution')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/memory_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"Memory Usage Statistics\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Average Real Storage: {df_job['real_storage'].mean() / 1024:.2f} MB\")\n",
    "    print(f\"Max Real Storage: {df_job['real_storage'].max() / 1024:.2f} MB\")\n",
    "    if 'virtual_storage' in df_job.columns:\n",
    "        print(f\"Average Virtual Storage: {df_job['virtual_storage'].mean() / 1024:.2f} MB\")\n",
    "        print(f\"Max Virtual Storage: {df_job['virtual_storage'].max() / 1024:.2f} MB\")\n",
    "    print(\"\\n✓ Memory visualization saved to: reports/memory_analysis.png\")\n",
    "else:\n",
    "    print(\"Memory fields not available in parsed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963bf17",
   "metadata": {},
   "source": [
    "## 9. Analyze I/O Operations (EXCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6427db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_interval.empty and 'excp_count' in df_interval.columns:\n",
    "    # EXCP Statistics\n",
    "    print(\"I/O (EXCP) Statistics\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total EXCP Count: {df_interval['excp_count'].sum():,}\")\n",
    "    print(f\"Average EXCP per job: {df_interval['excp_count'].mean():.2f}\")\n",
    "    print(f\"Max EXCP (single job): {df_interval['excp_count'].max():,}\")\n",
    "    \n",
    "    # Top I/O intensive jobs\n",
    "    print(\"\\n\\nTop 10 I/O Intensive Jobs\")\n",
    "    print(\"=\"*60)\n",
    "    top_io = df_interval.nlargest(10, 'excp_count')[['job_name', 'program_name', 'excp_count']]\n",
    "    print(top_io.to_string(index=False))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # EXCP Distribution\n",
    "    axes[0].hist(df_interval['excp_count'], bins=50, color='darkgreen', edgecolor='black')\n",
    "    axes[0].set_xlabel('EXCP Count')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('I/O Operations (EXCP) Distribution')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top I/O Consumers\n",
    "    top15_io = df_interval.nlargest(15, 'excp_count')\n",
    "    axes[1].barh(range(len(top15_io)), top15_io['excp_count'], color='darkred')\n",
    "    axes[1].set_yticks(range(len(top15_io)))\n",
    "    axes[1].set_yticklabels(top15_io['job_name'], fontsize=8)\n",
    "    axes[1].set_xlabel('EXCP Count')\n",
    "    axes[1].set_title('Top 15 I/O Intensive Jobs')\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('reports/io_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n✓ I/O visualization saved to: reports/io_analysis.png\")\n",
    "else:\n",
    "    print(\"EXCP count field not available in parsed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f1d68",
   "metadata": {},
   "source": [
    "## 10. Export Reports to CSV/Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7bd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reports directory if it doesn't exist\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "\n",
    "# Export each subtype to CSV\n",
    "for subtype, df in dataframes.items():\n",
    "    csv_file = f'reports/smf30_subtype{subtype}_report.csv'\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"✓ Exported Subtype {subtype}: {csv_file} ({len(df)} records)\")\n",
    "\n",
    "# Create summary Excel file with multiple sheets\n",
    "if dataframes:\n",
    "    excel_file = 'reports/smf30_complete_report.xlsx'\n",
    "    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "        for subtype, df in dataframes.items():\n",
    "            sheet_name = f'Subtype_{subtype}'\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"\\n✓ Created Excel report: {excel_file}\")\n",
    "    print(f\"  Contains {len(dataframes)} sheets\")\n",
    "\n",
    "print(\"\\n✓ All reports exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f860c",
   "metadata": {},
   "source": [
    "## 11. Custom Analysis Example\n",
    "\n",
    "Perform custom queries on the data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
